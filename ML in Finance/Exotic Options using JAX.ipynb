{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4791,
     "status": "ok",
     "timestamp": 1670191425713,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "ypSrQvxzipH4",
    "outputId": "8c02ff7b-cf9b-45cb-d14d-3f4e48876b8d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: optax in /usr/local/lib/python3.8/dist-packages (0.1.4)\n",
      "Requirement already satisfied: dm-haiku in /usr/local/lib/python3.8/dist-packages (0.0.9)\n",
      "Requirement already satisfied: jaxlib>=0.1.37 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25+cuda11.cudnn805)\n",
      "Requirement already satisfied: jax>=0.1.55 in /usr/local/lib/python3.8/dist-packages (from optax) (0.3.25)\n",
      "Requirement already satisfied: typing-extensions>=3.10.0 in /usr/local/lib/python3.8/dist-packages (from optax) (4.1.1)\n",
      "Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.8/dist-packages (from optax) (1.21.6)\n",
      "Requirement already satisfied: chex>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from optax) (0.1.5)\n",
      "Requirement already satisfied: absl-py>=0.7.1 in /usr/local/lib/python3.8/dist-packages (from optax) (1.3.0)\n",
      "Requirement already satisfied: dm-tree>=0.1.5 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.1.7)\n",
      "Requirement already satisfied: toolz>=0.9.0 in /usr/local/lib/python3.8/dist-packages (from chex>=0.1.5->optax) (0.12.0)\n",
      "Requirement already satisfied: scipy>=1.5 in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (1.7.3)\n",
      "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.8/dist-packages (from jax>=0.1.55->optax) (3.3.0)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.8/dist-packages (from dm-haiku) (0.8.10)\n",
      "Requirement already satisfied: jmp>=0.0.2 in /usr/local/lib/python3.8/dist-packages (from dm-haiku) (0.0.2)\n"
     ]
    }
   ],
   "source": [
    "pip install optax dm-haiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iolGFzFoccvU"
   },
   "source": [
    "Conversion Steps\n",
    "1. Numpy\n",
    "2. Jax normal eqn (chebyshev polynomials)\n",
    "<!-- 3. (2.5)Jax (normal polynomials) -->\n",
    "3. Jax (Normal Polynomials, gradient descent): we revert back to normal polynomaisl because we don't need chebyshev in neural networks\n",
    "4. Jax stochastic gradient *descent*\n",
    "5. Jax (Neueral Network, stochastic gradient descent)\n",
    "6. Price Exotic options"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Umss_PvMjmPG"
   },
   "source": [
    "Due date: October 21 2022 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UNMPZrw6ucMH"
   },
   "source": [
    "# Description\n",
    "  In this problem we will use apply the LSMC method to price American put options. Specifically, we will replicate the result in the first row, 6th column of Table 1 in [Longstaff and Schwartz 2001](https://www.anderson.ucla.edu/documents/areas/fac/finance/least_squares.pdf)\n",
    "\n",
    "  \n",
    "\n",
    "*  Read the introduction of the [paper](https://www.anderson.ucla.edu/documents/areas/fac/finance/least_squares.pdf).\n",
    "*   We will price an american put option as described in page 126 of the aforementioned article. Read paragraphs 1 and 2 of page 126\n",
    "* As we saw in class, one of the ways we can use linear regression to fit nonlinear functions is to use polynomial features. A common choice in many applications is to use the so called ``Chebyshev polynomials''. Chebyshev polynomials are defined recursively by:\n",
    "\n",
    "\\begin{equation}\n",
    "T_0(x) = 1\\\\\n",
    "T_1(x) = x\\\\\n",
    "T_{n + 1}(x)  =  2 x T_n(x) - T_{n - 1}(x)\\\\\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qszv8niMdzJK"
   },
   "source": [
    "# Part 1\n",
    "The code below simulates the evolution of a stock price that follows a geometric brownian motion. Write a JAX version of that code. You are not allowed to use functions from other libraries. For this part, the \"simulate\"\n",
    "function does not need to be jit compiled. As we will see, jit compiling a funciton with for loops may introduce some complications\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D6VdC2QZduv9"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Data\n",
    "σ = 0.04\n",
    "r = 0.01\n",
    "K = 35\n",
    "\n",
    "# Design choice\n",
    "dt = 0.01\n",
    "m = 100\n",
    "\n",
    "\n",
    "def simulate():\n",
    "  np.random.seed(0)\n",
    "\n",
    "  def step(S):\n",
    "    dZ = np.random.normal(size=S.size) * np.sqrt(dt)\n",
    "    dS = r * S  * dt + σ  * S  * dZ\n",
    "    S = S + dS\n",
    "    return S\n",
    "\n",
    "  S0 = np.ones(20000)\n",
    "  S = S0\n",
    "  S_list = []\n",
    "\n",
    "  for t in range(m):\n",
    "    S = step(S)\n",
    "    S_list.append(S)\n",
    "\n",
    "  S_array = np.stack(S_list)\n",
    "  return S_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8TLBJNOc4vW0"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Data\n",
    "σ = 0.04\n",
    "r = 0.01\n",
    "K = 35\n",
    "\n",
    "# Design choice\n",
    "dt = 0.01\n",
    "m = 100\n",
    "\n",
    "\n",
    "def simulate():\n",
    "  key = jnp.randomPRNGKey(0)\n",
    "\n",
    "  def step(S):\n",
    "    rng, _ = jax.random.split(rng) #psuedo random number generator. not really random but it makes it looks random\n",
    "    dZ = jnp.random.normal(rng, (S.size)) * np.sqrt(dt)\n",
    "    dS = r * S  * dt + σ  * S  * dZ\n",
    "    S = S + dS\n",
    "    return S\n",
    "\n",
    "  S0 = jnp.ones(20000)\n",
    "  S = S0\n",
    "  S_list = []\n",
    "\n",
    "  for t in range(m):\n",
    "    S = step(S)\n",
    "    S_list.append(S)\n",
    "\n",
    "  S_array = np.stack(S_list)\n",
    "  return S_array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-sYIUZ1QfF-i"
   },
   "source": [
    "# Part 2\n",
    "Write a jit compiled version of the simulate function. You may want to check out the function jax.lax.scan.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLwJNqg75O81"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "# Data\n",
    "σ = 0.04\n",
    "r = 0.01\n",
    "K = 35\n",
    "\n",
    "# Design choice\n",
    "dt = 0.01\n",
    "m = 100\n",
    "\n",
    "@jax.jit\n",
    "def simulate():\n",
    "  seed = jax.random.PRNGKey(0)\n",
    "\n",
    "  def core(state, input):\n",
    "    S = state\n",
    "    rng = input    \n",
    "    dZ = jax.random.normal(rng, (S.size, )) * np.sqrt(dt)\n",
    "    dS = r * S  * dt + σ  * S  * dZ\n",
    "    S = S + dS\n",
    "    return S, S #first S is final state, 2nd S is the vector or matrix of all states found\n",
    "\n",
    "  S0 = jnp.ones(int(m/dt))\n",
    "  S = S0\n",
    "  S_list = []\n",
    "\n",
    "  rng_vector = jax.random.split(seed, m) #m is the length of vector\n",
    "  state = S\n",
    "  input = rng_vector\n",
    "\n",
    "  state, out = jax.lax.scan(core, state, input)\n",
    "\n",
    "  return state, out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 484,
     "status": "ok",
     "timestamp": 1669955575003,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "pFOfQ6di6kJ9",
    "outputId": "e59901b2-d0be-4b1e-ce91-1ddd7551eaf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([0.9991813, 1.066116 , 0.9695461, ..., 0.9957512, 1.0195965,\n",
       "              0.9298693], dtype=float32),\n",
       " DeviceArray([[0.99995965, 1.0011096 , 0.99595296, ..., 0.9970569 ,\n",
       "               1.0003599 , 0.99666893],\n",
       "              [1.0031309 , 1.0017953 , 0.9969345 , ..., 0.9964643 ,\n",
       "               1.0038731 , 1.0018477 ],\n",
       "              [1.0038399 , 1.0019187 , 0.99650645, ..., 0.99268675,\n",
       "               1.005771  , 1.006995  ],\n",
       "              ...,\n",
       "              [1.0062256 , 1.066818  , 0.9617757 , ..., 0.9942317 ,\n",
       "               1.0212085 , 0.93702674],\n",
       "              [1.003928  , 1.067885  , 0.96726656, ..., 0.99592817,\n",
       "               1.0197272 , 0.9276689 ],\n",
       "              [0.9991813 , 1.066116  , 0.9695461 , ..., 0.9957512 ,\n",
       "               1.0195965 , 0.9298693 ]], dtype=float32))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YdCFrHUafwBh"
   },
   "source": [
    "# Part 3\n",
    "The code below is computes the price of an American Put option using Least Squares Monte Carlo (LSMC). Write a JAX version of that code. You are not allowed to use functions from other libraries. Your \"compute_price\" function must be jit compiled.\n",
    "\n",
    "1. Numpy version of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3073,
     "status": "ok",
     "timestamp": 1669955579793,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "8r1gdoyiAqSy",
    "outputId": "2b7bf917-573a-478c-c1f1-623decd28d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.460566940166749\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "Spot = 36   # stock price\n",
    "σ = 0.2     # stock volatility\n",
    "K = 40      # strike price\n",
    "r = 0.06    # risk free rate\n",
    "n = 100000  # Number of simualted paths\n",
    "m = 100      # number of exercise dates\n",
    "T = 1       # maturity\n",
    "order = 12   # Polynmial order\n",
    "Δt = T / m  # interval between two exercise dates\n",
    "\n",
    "\n",
    "# Construct polynomial features of order up to k using the\n",
    "# recursive formulation\n",
    "def chebyshev_basis(x, k):\n",
    "    B = [np.ones(len(x)), x]\n",
    "    for n in range(2, k):\n",
    "        Bn = 2 * x * B[n - 1] - B[n - 2]\n",
    "        B.append(Bn)\n",
    "\n",
    "    return np.column_stack(B)\n",
    "\n",
    "\n",
    "# scales x to be in the interval(-1, 1)\n",
    "def scale(x):\n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    a = 2 / (xmax - xmin)\n",
    "    b = 1 - a * xmax\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# simulates one step of the stock price evolution\n",
    "def step(S):\n",
    "    dB = np.sqrt(Δt) * np.random.normal(size=S.size)\n",
    "    S_tp1 = S + r * S * Δt + σ * S * dB\n",
    "    return S_tp1\n",
    "\n",
    "\n",
    "def payoff_put(S):\n",
    "    return np.maximum(K - S, 0.)\n",
    "\n",
    "\n",
    "# LSMC algorithm\n",
    "def compute_price():\n",
    "    np.random.seed(0)\n",
    "    S0 = Spot * np.ones(n)\n",
    "    S = [S0]\n",
    "\n",
    "    for t in range(m):\n",
    "        S_tp1 = step(S[t])\n",
    "        S.append(S_tp1)\n",
    "\n",
    "    discount = np.exp(-r * Δt)\n",
    "\n",
    "    # Very last date\n",
    "    value_if_exercise = payoff_put(S[-1])\n",
    "    discounted_future_cashflows = value_if_exercise * discount\n",
    "\n",
    "    # Proceed recursively\n",
    "    for i in range(m - 1):\n",
    "        X = chebyshev_basis(scale(S[-2 - i]), order)\n",
    "        Y = discounted_future_cashflows\n",
    "\n",
    "        Θ = np.linalg.solve(X.T @ X, X.T @ Y)\n",
    "        value_if_wait = X @ Θ\n",
    "        value_if_exercise = payoff_put(S[-2 - i])\n",
    "        exercise = value_if_exercise >= value_if_wait\n",
    "        discounted_future_cashflows = discount * np.where(\n",
    "            exercise,\n",
    "            value_if_exercise,\n",
    "            discounted_future_cashflows)\n",
    "\n",
    "    return discounted_future_cashflows.mean()\n",
    "\n",
    "\n",
    "print(compute_price())\n",
    "# test = compute_price(order, Spot, σ, K, r)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hA54nr9BPTi0"
   },
   "source": [
    "2. Jax version of the code using chebyshev polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3042,
     "status": "ok",
     "timestamp": 1669955593345,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "w539_qu_fFaf",
    "outputId": "99102801-5b61-49bf-bdea-c64dd1745e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.469381\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "\n",
    "Spot = 36   # stock price\n",
    "σ = 0.2     # stock volatility\n",
    "K = 40      # strike price\n",
    "r = 0.06    # risk free rate\n",
    "n = 100000  # Number of simualted paths\n",
    "m = 100      # number of exercise dates\n",
    "T = 1       # maturity\n",
    "order = 12   # Polynmial order\n",
    "Δt = T / m  # interval between two exercise dates\n",
    "\n",
    "\n",
    "# Construct polynomial features of order up to k using the\n",
    "# recursive formulation\n",
    "def chebyshev_basis(x, k):\n",
    "    B = [jnp.ones(len(x)), x]\n",
    "    for n in range(2, k):\n",
    "        Bn = 2 * x * B[n - 1] - B[n - 2]\n",
    "        B.append(Bn)\n",
    "\n",
    "    return jnp.column_stack(B)\n",
    "\n",
    "\n",
    "# scales x to be in the interval(-1, 1)\n",
    "def scale(x):\n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    a = 2 / (xmax - xmin)\n",
    "    b = 1 - a * xmax\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# simulates one step of the stock price evolution\n",
    "def step(S, rng):\n",
    "    e = jax.random.normal(rng, shape=S.shape)\n",
    "    dB = jnp.sqrt(Δt) * e\n",
    "    S = S + r * S * Δt + σ * S * dB\n",
    "    return S, S\n",
    "\n",
    "def payoff_put(S):\n",
    "    return jnp.maximum(K - S, 0.)\n",
    "\n",
    "@jax.jit\n",
    "# LSMC algorithm\n",
    "def compute_price():\n",
    "    seed = jax.random.PRNGKey(0) \n",
    "    S = Spot * jnp.ones(int(n))\n",
    "    \n",
    "    rng_vector = jax.random.split(seed, int(m))\n",
    "    state = S\n",
    "    input = rng_vector #1st dimension of input needs to have same amount of steps to loop in the function\n",
    "    _, S = jax.lax.scan(step, state, input) #the S = 100 x 100000 matrix\n",
    "\n",
    "    discount = jnp.exp(-r * Δt)\n",
    "\n",
    "    # Very last date\n",
    "    value_if_exercise = payoff_put(S[-1])\n",
    "    discounted_future_cashflows = value_if_exercise * discount\n",
    "    \n",
    "    def core(state, input):\n",
    "      Si = input\n",
    "      discounted_future_cashflows = state\n",
    "\n",
    "      X = chebyshev_basis(scale(Si), order)\n",
    "      Y = discounted_future_cashflows\n",
    "      Θ = jnp.linalg.solve(X.T @ X, X.T @ Y)  # this line sets up the loss function and solve it by minimizing it\n",
    "      value_if_wait = X @ Θ\n",
    "      value_if_exercise = payoff_put(Si)\n",
    "      exercise = value_if_exercise >= value_if_wait\n",
    "      discounted_future_cashflows = discount * jnp.where(\n",
    "          exercise,\n",
    "          value_if_exercise,\n",
    "          discounted_future_cashflows)\n",
    "\n",
    "      return discounted_future_cashflows, None\n",
    "\n",
    "    # we are reversing the order because we need to go backwards in time\n",
    "    input = jnp.flip(S, 0)[1:] #pass theta as an input to core function because we need to pass diff Θ for every time step\n",
    "    state = discounted_future_cashflows\n",
    "    discounted_future_cashflows, _ = jax.lax.scan(core, state, input)\n",
    "\n",
    "    # for i in range(m - 1):\n",
    "    #   discounted_future_cashflows, _ = core(discounted_future_cashflows, input[i])  \n",
    "    \n",
    "    return discounted_future_cashflows.mean()\n",
    "\n",
    "print(compute_price())\n",
    "# test = compute_price(order, Spot, σ, K, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e1jDUPIcP7Eg"
   },
   "source": [
    "3. Jax Gradient descent (normal polynomials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoHdxSPhb8Ct"
   },
   "outputs": [],
   "source": [
    "# Step 1 of grad descent is to initialize Θ with 99 x 12(order of polynomial terms) columns\n",
    "# Θ = Θ- learning_rate * delL\n",
    "# Stock simulation stays the same in grad desc\n",
    "#regress futurre cahsflow on future S[98]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 988,
     "status": "ok",
     "timestamp": 1670141263789,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "Q3QeczG9P0x3",
    "outputId": "8677190b-6108-4421-e960-2cdcda3a7144"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(3.9540339, dtype=float32), DeviceArray([37.68034 , 37.37557 , 37.100048, 36.81555 , 36.519394,\n",
      "             36.196728, 35.936237, 35.66297 , 35.36962 , 35.0462  ,\n",
      "             34.7042  , 34.42286 , 34.073692, 33.78233 , 33.42189 ,\n",
      "             33.036335, 32.716248, 32.342255, 31.92985 , 31.629984,\n",
      "             31.293911, 31.001877, 30.602314, 30.206573, 29.85009 ,\n",
      "             29.468176, 29.110407, 28.67713 , 28.275558, 27.83037 ,\n",
      "             27.404428, 26.93171 , 26.48799 , 26.007463, 25.574371,\n",
      "             25.080164, 24.610832, 24.116224, 23.58388 , 23.024042,\n",
      "             22.501335, 21.925869, 21.309639, 20.723457, 20.11183 ,\n",
      "             19.439226, 18.764383, 18.062632, 17.374859], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "Spot = 36   # stock price\n",
    "σ = 0.2     # stock volatility\n",
    "K = 40      # strike price\n",
    "r = 0.06    # risk free rate\n",
    "n = 100000  # Number of simualted paths # we dont need so many paths if we are using stochastic gradient descent\n",
    "m = 50      # number of exercise dates\n",
    "T = 1       # maturity\n",
    "order = 5   # Polynmial order. Increase value and see what happens\n",
    "Δt = T / m  # interval between two exercise dates\n",
    "\n",
    "optimizer = optax.adam #we/re using optax to optimize gradient descent\n",
    "α=1e-3 # learning rate\n",
    "\n",
    "# # Construct polynomial features of order up to k using the\n",
    "# # recursive formulation\n",
    "# def chebyshev_basis(x, k):\n",
    "#     B = [jnp.ones(len(x)), x]\n",
    "#     for n in range(2, k):\n",
    "#         Bn = 2 * x * B[n - 1] - B[n - 2]\n",
    "#         B.append(Bn)\n",
    "\n",
    "#     return jnp.column_stack(B)\n",
    "\n",
    "\n",
    "# scales x to be in the interval(-1, 1)\n",
    "def scale(x):\n",
    "  xmin = x.min()\n",
    "  xmax = x.max()\n",
    "  a = 2 / (xmax - xmin)\n",
    "  b = 1 - a * xmax\n",
    "  return a * x + b\n",
    "\n",
    "\n",
    "# simulates one step of the stock price evolution\n",
    "def step(S, rng):\n",
    "  e = jax.random.normal(rng, shape=S.shape)\n",
    "  dB = jnp.sqrt(Δt) * e\n",
    "  S = S + r * S * Δt + σ * S * dB\n",
    "  return S, S\n",
    "\n",
    "def payoff_put(S):\n",
    "  return jnp.maximum(K - S, 0.)\n",
    "\n",
    "#Step 1 of grad descent\n",
    "Θ = jnp.zeros((m-1, order)) # Theta is 99x order filled with zeroes. This is our initial guess for theta(wrong values. used just to see what happens)\n",
    "opt_state = optimizer(1.).init(Θ) # initializing optimizer state\n",
    "\n",
    "def mse(predictions, label):\n",
    "  return          \n",
    "\n",
    "@jax.jit\n",
    "# LSMC algorithm\n",
    "def compute_price(Θ): # Step 2: pass theta as input to function because it is an iterative solution dependent on Θ\n",
    "  seed = jax.random.PRNGKey(0)\n",
    "  S = Spot * jnp.ones(int(n))\n",
    "  \n",
    "  rng_vector = jax.random.split(seed, int(m))\n",
    "  state = S\n",
    "  input = rng_vector #1st dimension of input needs to have same amount of steps to loop in the function\n",
    "  _, S = jax.lax.scan(step, state, input) #the S = 100 x 100000 matrix\n",
    "\n",
    "  discount = jnp.exp(-r * Δt)\n",
    "\n",
    "  # Very last date\n",
    "  value_if_exercise = payoff_put(S[-1])\n",
    "  discounted_future_cashflows = value_if_exercise * discount\n",
    "\n",
    "  def core(state, input):\n",
    "    Si, Θi = input #we take in Θ as input because it keeps changing for every time  when computing mse\n",
    "    discounted_future_cashflows = state\n",
    "\n",
    "    X = jnp.column_stack([Si**k for k in range(order)])\n",
    "    # X = chebyshev_basis(scale(Si), order)\n",
    "    Y = discounted_future_cashflows\n",
    "    \n",
    "    \n",
    "    # Θ = jnp.linalg.solve(X.T @ X, X.T @ Y)  \n",
    "    # above line sets up the loss function and solve it by minimizing it but we only want the mse\n",
    "    \n",
    "    #Below line says continuation value is features matrix * the initial guessed Θ values\n",
    "    value_if_wait = X @ Θi # Theta becomes Θi compared to previous step. This doesnt change from step to step but each step needs a different Θ\n",
    "    \n",
    "    # here we are finding the mse but not solving(minimizing it)\n",
    "    mse_i = jnp.mean((value_if_wait-discounted_future_cashflows)**2) \n",
    "    \n",
    "  \n",
    "    value_if_exercise = payoff_put(Si)\n",
    "    exercise = value_if_exercise >= value_if_wait\n",
    "    discounted_future_cashflows = discount * jnp.where(exercise, value_if_exercise, discounted_future_cashflows)\n",
    "\n",
    "    return discounted_future_cashflows, mse_i #second output is a time series of mean square errors\n",
    "    #we need to keep track of mse because we need minimize the sum and find mean of mse in our Loss function\n",
    "\n",
    "  input = jnp.flip(S, 0)[1:], Θ #pass theta as an input to core function because we need to pass diff Θ for every time step\n",
    "  state = discounted_future_cashflows\n",
    "  discounted_future_cashflows, mse_i = jax.lax.scan(core, state, input) #second output would be a vector of mse\n",
    "\n",
    "  # for i in range(m - 1):\n",
    "  #   discounted_future_cashflows, _ = core(discounted_future_cashflows, input[i])  \n",
    "  \n",
    "  return discounted_future_cashflows.mean(), mse_i # mse_i is a vector containing 99 values of the mse for each time step except the last time stamp\n",
    "\n",
    "print(compute_price(Θ)) #calculates the price using the guessed(wrong) \n",
    "# Θ\n",
    "\n",
    "#each value in the 99 dimensional vector is the mse of the time stamp\n",
    "# test = compute_price(order, Spot, σ, K, r)\n",
    "\n",
    "# observe the price fell to 3.97 from 4.46 because we removed chebyshev polynomials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5935,
     "status": "ok",
     "timestamp": 1670141270130,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "2DfuI7VqAUsq",
    "outputId": "905521dc-e03a-4d72-e5b5-ce5f339c15b0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8474746\n",
      "4.44126\n",
      "4.1777544\n",
      "4.2251863\n",
      "4.2377887\n",
      "4.242069\n",
      "4.243798\n",
      "4.2425575\n",
      "4.2410126\n",
      "4.2380733\n"
     ]
    }
   ],
   "source": [
    "@jax.jit #because we want it to run quick\n",
    "def update_gradient_descent(Θ, opt_state): \n",
    "\n",
    "  def L(Θ): #setting up the Loss function after preparing our above code to get mse vector is good practice\n",
    "    _, mse_i = compute_price(Θ)\n",
    "    return mse_i.sum()\n",
    "\n",
    "  grad = jax.grad(L)(Θ) #compute gradient of L at previous parameter Θ\n",
    "  updates, opt_state = optimizer(α).update(grad, opt_state) # call optimizer at learning rate and give in the gradient and the previous optimizer state. \n",
    "                                                            # that function returns updates with new optimizer state\n",
    "  \n",
    "  Θ = optax.apply_updates(Θ, updates) #compute new parameter Θ\n",
    "  return Θ, opt_state \n",
    "\n",
    "@jax.jit\n",
    "def evaluate(Θ):\n",
    "  p, _ = compute_price(Θ) #we can observe the jump is option price because the Theta values are more optimized\n",
    "  return p\n",
    "\n",
    "for iteration in range(1000): #change range to 10000 and see what happens\n",
    "  Θ, opt_state = update_gradient_descent(Θ, opt_state)\n",
    "\n",
    "  if iteration % 100 == 0:\n",
    "    print(evaluate(Θ))\n",
    "\n",
    "#Why this specific pattern in the output?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KgMZ5ZM_vbXN"
   },
   "source": [
    "Very slow becauae at each iteration we are computing 100000 price paths and we simulate 100 peers. and we waste alot of computational power to perform 1 step of gradient descent.\n",
    "\n",
    "Instead of 100000 simulations, we can simulate 500 bu using stochastic gradient descent. Also we know in principal stochastic gradient descent works like gradient descent but much much faster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9XaiMSsxvDF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7rVAh3Xx6bt"
   },
   "source": [
    "Lets see why chebyshev polunomials wont work in stochastic gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dR1Y13Vpx30A"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "Spot = 36   # stock price\n",
    "σ = 0.2     # stock volatility\n",
    "K = 40      # strike price\n",
    "r = 0.06    # risk free rate\n",
    "n = 100000  # Number of simualted paths # we dont need so many paths if we are using stochastic gradient descent\n",
    "m = 50      # number of exercise dates\n",
    "batch_size = 512 #batch size\n",
    "T = 1       # maturity\n",
    "order = 5   # Polynmial order. Increase value and see what happens\n",
    "Δt = T / m  # interval between two exercise dates\n",
    "\n",
    "optimizer = optax.adam #we/re using optax to optimize gradient descent\n",
    "α=1e-3 # learning rate\n",
    "\n",
    "# Construct polynomial features of order up to k using the\n",
    "# recursive formulation\n",
    "def chebyshev_basis(x, k):\n",
    "    B = [jnp.ones(len(x)), x]\n",
    "    for n in range(2, k):\n",
    "        Bn = 2 * x * B[n - 1] - B[n - 2]\n",
    "        B.append(Bn)\n",
    "\n",
    "    return jnp.column_stack(B)\n",
    "\n",
    "\n",
    "# scales x to be in the interval(-1, 1)\n",
    "def scale(x):\n",
    "  xmin = 10 #x.min()\n",
    "  xmax = 70 #x.max()\n",
    "  a = 2 / (xmax - xmin)\n",
    "  b = 1 - a * xmax\n",
    "  return a * x + b\n",
    "\n",
    "\n",
    "# simulates one step of the stock price evolution\n",
    "def step(S, rng):\n",
    "  e = jax.random.normal(rng, shape=S.shape)\n",
    "  dB = jnp.sqrt(Δt) * e\n",
    "  S = S + r * S * Δt + σ * S * dB\n",
    "  return S, S\n",
    "\n",
    "def payoff_put(S):\n",
    "  return jnp.maximum(K - S, 0.)\n",
    "\n",
    "#Step 1 of grad descent\n",
    "Θ = jnp.zeros((m-1, order)) \n",
    "opt_state = optimizer(1.).init(Θ)\n",
    "\n",
    "def model(Θi, Si):\n",
    "  X = chebyshev_basis(scale(Si), order)\n",
    "  value_if_wait = X @ Θi\n",
    "  return value_if_wait\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# LSMC algorithm\n",
    "def compute_price(Θ, n, seed): #pass seed because it is SGD\n",
    "  S = Spot * jnp.ones(n)\n",
    "  \n",
    "  rng_vector = jax.random.split(seed, int(m))\n",
    "  state = S\n",
    "  input = rng_vector \n",
    "  _, S = jax.lax.scan(step, state, input) \n",
    "\n",
    "  discount = jnp.exp(-r * Δt)\n",
    "\n",
    "  value_if_exercise = payoff_put(S[-1])\n",
    "  discounted_future_cashflows = value_if_exercise * discount\n",
    "\n",
    "  def core(state, input):\n",
    "    Si, Θi = input\n",
    "    discounted_future_cashflows = state\n",
    "    Y = discounted_future_cashflows\n",
    "\n",
    "    #Below line says continuation value is features matrix * the initial guessed Θ values\n",
    "    # value_if_wait = X @ Θi\n",
    "    value_if_wait = model(Θi, Si)\n",
    "    \n",
    "    mse_i = jnp.mean((value_if_wait-discounted_future_cashflows)**2) \n",
    "    \n",
    "    value_if_exercise = payoff_put(Si)\n",
    "    exercise = value_if_exercise >= value_if_wait\n",
    "    discounted_future_cashflows = discount * jnp.where(exercise, value_if_exercise, discounted_future_cashflows)\n",
    "\n",
    "    return discounted_future_cashflows, mse_i\n",
    "  \n",
    "  input = jnp.flip(S, 0)[1:], Θ\n",
    "  state = discounted_future_cashflows\n",
    "  discounted_future_cashflows, mse_i = jax.lax.scan(core, state, input)\n",
    "\n",
    "  return discounted_future_cashflows.mean(), mse_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 73609,
     "status": "error",
     "timestamp": 1670141611011,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "jJJlQkY0F_Gt",
    "outputId": "063a2a8e-b877-46ee-d962-3ec042a644a2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9511049\n",
      "3.951169\n",
      "3.9513664\n",
      "3.952066\n",
      "3.9534402\n",
      "3.9558983\n",
      "3.9602818\n",
      "3.9675856\n",
      "3.9780893\n",
      "3.9932992\n",
      "4.0120234\n",
      "4.034157\n",
      "4.05979\n",
      "4.086995\n",
      "4.115958\n",
      "4.14497\n",
      "4.170677\n",
      "4.1953187\n",
      "4.217897\n",
      "4.238947\n",
      "4.257911\n",
      "4.2746468\n",
      "4.2887616\n",
      "4.3000746\n",
      "4.310746\n",
      "4.318579\n",
      "4.3243866\n",
      "4.3289695\n",
      "4.3338013\n",
      "4.3364935\n",
      "4.3383493\n",
      "4.3393617\n",
      "4.341795\n",
      "4.342681\n",
      "4.3453813\n",
      "4.3457966\n",
      "4.3456273\n",
      "4.347289\n",
      "4.348111\n",
      "4.3473597\n",
      "4.348216\n",
      "4.347362\n",
      "4.347083\n",
      "4.344932\n",
      "4.3432746\n",
      "4.343918\n",
      "4.343767\n",
      "4.3406205\n",
      "4.3389845\n",
      "4.336547\n",
      "4.335172\n",
      "4.3335967\n",
      "4.3317304\n",
      "4.328073\n",
      "4.3243384\n",
      "4.322855\n",
      "4.3195524\n",
      "4.3152437\n",
      "4.311239\n",
      "4.3100977\n",
      "4.3060117\n",
      "4.3020735\n",
      "4.295375\n",
      "4.293712\n",
      "4.29027\n",
      "4.2847056\n",
      "4.279872\n",
      "4.2785397\n",
      "4.270176\n",
      "4.2672606\n",
      "4.2636366\n",
      "4.2595825\n",
      "4.255797\n",
      "4.2507925\n",
      "4.245539\n",
      "4.242451\n",
      "4.239733\n",
      "4.2311397\n",
      "4.234604\n",
      "4.226414\n",
      "4.2255254\n",
      "4.2251186\n",
      "4.224458\n",
      "4.217593\n",
      "4.22411\n",
      "4.2217035\n",
      "4.223073\n",
      "4.2219205\n",
      "4.2216644\n",
      "4.2205257\n",
      "4.220975\n",
      "4.220631\n",
      "4.221236\n",
      "4.220102\n",
      "4.2212844\n",
      "4.2224836\n",
      "4.2217298\n",
      "4.222287\n",
      "4.222148\n",
      "4.221369\n",
      "4.226428\n",
      "4.227549\n",
      "4.2298293\n",
      "4.2306657\n",
      "4.2279997\n",
      "4.231545\n",
      "4.23584\n",
      "4.238748\n",
      "4.2392235\n",
      "4.243594\n",
      "4.2406263\n",
      "4.2451057\n",
      "4.242687\n",
      "4.242252\n",
      "4.241024\n",
      "4.2476506\n",
      "4.246027\n",
      "4.24842\n",
      "4.2504916\n",
      "4.2532134\n",
      "4.2584205\n",
      "4.2597\n",
      "4.2617555\n",
      "4.260396\n",
      "4.2635207\n",
      "4.2667036\n",
      "4.2707\n",
      "4.269384\n",
      "4.2703347\n",
      "4.2734575\n",
      "4.2753787\n",
      "4.2739367\n",
      "4.277237\n",
      "4.277225\n",
      "4.280482\n",
      "4.276657\n",
      "4.278782\n",
      "4.282627\n",
      "4.2859445\n",
      "4.280751\n",
      "4.289081\n",
      "4.291062\n",
      "4.288252\n",
      "4.2913895\n",
      "4.2904744\n",
      "4.291101\n",
      "4.2962704\n",
      "4.299973\n",
      "4.299315\n",
      "4.3012385\n",
      "4.3004856\n",
      "4.3044047\n",
      "4.304182\n",
      "4.3055105\n",
      "4.3063545\n",
      "4.3050475\n",
      "4.308143\n",
      "4.3101907\n",
      "4.3122406\n",
      "4.31213\n",
      "4.3152165\n",
      "4.3153896\n",
      "4.31533\n",
      "4.3202057\n",
      "4.320471\n",
      "4.317372\n",
      "4.319381\n",
      "4.3209715\n",
      "4.3198237\n",
      "4.323898\n",
      "4.326193\n",
      "4.330442\n",
      "4.328665\n",
      "4.332064\n",
      "4.3300114\n",
      "4.3318424\n",
      "4.331025\n",
      "4.3305287\n",
      "4.332511\n",
      "4.3350053\n",
      "4.3352184\n",
      "4.335199\n",
      "4.335815\n",
      "4.3378663\n",
      "4.3379583\n",
      "4.3404927\n",
      "4.339347\n",
      "4.341615\n",
      "4.3434906\n",
      "4.3444967\n",
      "4.34375\n",
      "4.3435144\n",
      "4.346462\n",
      "4.344952\n",
      "4.344083\n",
      "4.347629\n",
      "4.350104\n",
      "4.349871\n",
      "4.3486056\n",
      "4.349255\n",
      "4.3514247\n",
      "4.3515034\n",
      "4.352273\n",
      "4.3523026\n",
      "4.353074\n",
      "4.3521137\n",
      "4.3540206\n",
      "4.354597\n",
      "4.355482\n",
      "4.352491\n",
      "4.356653\n",
      "4.358075\n",
      "4.3571205\n",
      "4.3575435\n",
      "4.3563304\n",
      "4.355473\n",
      "4.3581963\n",
      "4.3592257\n",
      "4.357024\n",
      "4.359795\n",
      "4.360923\n",
      "4.3601046\n",
      "4.3609147\n",
      "4.3614893\n",
      "4.3613577\n",
      "4.3608904\n",
      "4.362663\n",
      "4.365364\n",
      "4.3671074\n",
      "4.3653073\n",
      "4.3664885\n",
      "4.364846\n",
      "4.3647966\n",
      "4.3660607\n",
      "4.364131\n",
      "4.3643956\n",
      "4.3667507\n",
      "4.3693166\n",
      "4.3700294\n",
      "4.369953\n",
      "4.3690977\n",
      "4.371434\n",
      "4.370774\n",
      "4.3704457\n",
      "4.3730083\n",
      "4.37414\n",
      "4.3725467\n",
      "4.3756323\n",
      "4.371722\n",
      "4.372597\n",
      "4.374485\n",
      "4.3739443\n",
      "4.374613\n",
      "4.3747964\n",
      "4.372853\n",
      "4.3731284\n",
      "4.373085\n",
      "4.3736134\n",
      "4.3737426\n",
      "4.374675\n",
      "4.3716416\n",
      "4.3734875\n",
      "4.3721385\n",
      "4.3735304\n",
      "4.374459\n",
      "4.3737173\n",
      "4.375022\n",
      "4.376697\n",
      "4.3764205\n",
      "4.3775344\n",
      "4.377356\n",
      "4.3770204\n",
      "4.378672\n",
      "4.378329\n",
      "4.3793344\n",
      "4.37699\n",
      "4.37624\n",
      "4.3780766\n",
      "4.3771796\n",
      "4.375475\n",
      "4.378027\n",
      "4.3777113\n",
      "4.379309\n",
      "4.3791966\n",
      "4.3789067\n",
      "4.377879\n",
      "4.380431\n",
      "4.38347\n",
      "4.3856435\n",
      "4.382395\n",
      "4.378903\n",
      "4.381686\n",
      "4.3818417\n",
      "4.379449\n",
      "4.381163\n",
      "4.38224\n",
      "4.3849916\n",
      "4.3840523\n",
      "4.382211\n",
      "4.3805804\n",
      "4.3822784\n",
      "4.3817105\n",
      "4.381465\n",
      "4.380237\n",
      "4.380754\n",
      "4.3830075\n",
      "4.3841853\n",
      "4.384126\n",
      "4.384308\n",
      "4.3842835\n",
      "4.3836484\n",
      "4.3854647\n",
      "4.3848186\n",
      "4.3868055\n",
      "4.3844976\n",
      "4.382833\n",
      "4.384525\n",
      "4.384123\n",
      "4.3871555\n",
      "4.3838387\n",
      "4.3846555\n",
      "4.3859386\n",
      "4.385708\n",
      "4.385482\n",
      "4.3861938\n",
      "4.3872733\n",
      "4.38559\n",
      "4.385774\n",
      "4.3876038\n",
      "4.388626\n",
      "4.3874273\n",
      "4.388618\n",
      "4.386929\n",
      "4.387317\n",
      "4.389458\n",
      "4.3885837\n",
      "4.387196\n",
      "4.3884673\n",
      "4.387611\n",
      "4.3888254\n",
      "4.3880672\n",
      "4.3885784\n",
      "4.389062\n",
      "4.388789\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-b258246b041c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m#change range to 10000 and see what happens\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m   \u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "@jax.jit\n",
    "def update_gradient_descent(Θ, opt_state, seed): \n",
    "\n",
    "  seed, _ = jax.random.split(seed)\n",
    "\n",
    "  def L(Θ):\n",
    "    _, mse_i = compute_price(Θ, batch_size, seed)\n",
    "    return mse_i.sum()\n",
    "\n",
    "  grad = jax.grad(L)(Θ)\n",
    "  updates, opt_state = optimizer(α).update(grad, opt_state) \n",
    "                                                            \n",
    "  \n",
    "  Θ = optax.apply_updates(Θ, updates)\n",
    "  return Θ, opt_state, seed \n",
    "\n",
    "@jax.jit\n",
    "def evaluate(Θ):\n",
    "  seed = jax.random.PRNGKey(0) #doesnt matter if we use the same seed for evaluation\n",
    "  p, _ = compute_price(Θ, 200000, seed)\n",
    "  return p\n",
    "\n",
    "seed = jax.random.PRNGKey(0)\n",
    "\n",
    "for iteration in range(1000000):\n",
    "  Θ, opt_state, seed = update_gradient_descent(Θ, opt_state, seed)\n",
    "\n",
    "  if iteration % 100 == 0:\n",
    "    print(evaluate(Θ))\n",
    "\n",
    "# We can observe the predicted values gets better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fOcRtytYxvhy"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gbRfjX5VIrAg"
   },
   "source": [
    "Skip this step for now, we figured chebyshev works with higher number of iterations\n",
    "\n",
    "4. Jax Stochastic gradient descent (normal polynomials)\n",
    "\n",
    "We use regular polynomialls because .\n",
    "But in stochastic gradient descent minimum and maximum are gonna keep changing at each iteration\n",
    "\n",
    "Each time we call compute_price, we are gonna be sampling different shocks. If we use the code the way it is\n",
    "Different simulations will have different Si. At each step chebyshev scales the features at the maximum or minimum and this needs to be done in order to use chebyshev. If this scaling depends on our sample, it means the scaling is changing from iteration to iteration. But the scaling should be the same throughout trading. So it wont work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1704,
     "status": "ok",
     "timestamp": 1669944166729,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "xd5ou4BzEznh",
    "outputId": "fecc7fdf-71dd-409a-b8c6-af004559dea9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(DeviceArray(3.9780037, dtype=float32), DeviceArray([37.56345 , 37.446575, 37.298405, 37.1356  , 36.979633,\n",
      "             36.826267, 36.686455, 36.54746 , 36.394558, 36.257782,\n",
      "             36.110565, 35.97117 , 35.787395, 35.635506, 35.44384 ,\n",
      "             35.30973 , 35.17018 , 35.002453, 34.83765 , 34.706955,\n",
      "             34.56372 , 34.382797, 34.222466, 34.06043 , 33.905365,\n",
      "             33.803722, 33.628613, 33.461033, 33.312878, 33.162132,\n",
      "             33.00243 , 32.852543, 32.698105, 32.52248 , 32.31892 ,\n",
      "             32.16418 , 31.971344, 31.765043, 31.56413 , 31.387058,\n",
      "             31.221035, 31.062067, 30.920769, 30.766369, 30.558311,\n",
      "             30.365704, 30.179184, 30.008032, 29.82569 , 29.63478 ,\n",
      "             29.436325, 29.24955 , 29.032795, 28.83644 , 28.643064,\n",
      "             28.453009, 28.260712, 28.02322 , 27.795649, 27.562605,\n",
      "             27.350769, 27.146765, 26.931189, 26.725199, 26.524939,\n",
      "             26.291368, 26.039965, 25.834959, 25.605137, 25.388474,\n",
      "             25.15038 , 24.871094, 24.629509, 24.33749 , 24.06025 ,\n",
      "             23.80884 , 23.57631 , 23.338879, 23.06097 , 22.774387,\n",
      "             22.45397 , 22.16899 , 21.905647, 21.602615, 21.282644,\n",
      "             21.001272, 20.73783 , 20.391975, 20.094118, 19.784864,\n",
      "             19.439577, 19.083527, 18.738953, 18.407337, 18.085564,\n",
      "             17.745535, 17.39468 , 17.04179 , 16.69123 ], dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "\n",
    "Spot = 36   # stock price\n",
    "σ = 0.2     # stock volatility\n",
    "K = 40      # strike price\n",
    "r = 0.06    # risk free rate\n",
    "n = 100000  # Number of simualted paths # we dont need so many paths if we are using stochastic gradient descent\n",
    "m = 100      # number of exercise \n",
    "batch_size = 512\n",
    "T = 1       # maturity\n",
    "order = 5   # Polynmial order\n",
    "Δt = T / m  # interval between two exercise dates\n",
    "optimizer = optax.adam #we/re using optax to optimize gradient descent\n",
    "\n",
    "\n",
    "# scales x to be in the interval(-1, 1)\n",
    "def scale(x):\n",
    "    xmin = x.min()\n",
    "    xmax = x.max()\n",
    "    a = 2 / (xmax - xmin)\n",
    "    b = 1 - a * xmax\n",
    "    return a * x + b\n",
    "\n",
    "\n",
    "# simulates one step of the stock price evolution\n",
    "def step(S, rng):\n",
    "  e = jax.random.normal(rng, shape=S.shape)\n",
    "  dB = jnp.sqrt(Δt) * e\n",
    "  S = S + r * S * Δt + σ * S * dB\n",
    "  return S, S\n",
    "\n",
    "def payoff_put(S):\n",
    "  return jnp.maximum(K - S, 0.)\n",
    "\n",
    "\n",
    "#Step 1 of grad descent\n",
    "Θ = jnp.zeros((m-1, order)) # Theta is 99x5 filled with zeroes. This is our initial guess for theta\n",
    "opt_state = optimizer(1.).init(Θ)\n",
    "\n",
    "def mse(predictions, label):\n",
    "  return jnp.mean((predictions-label)**2)         \n",
    "\n",
    "\n",
    "seed = jax.random.PRNGKey(0) #we remove it outside from the compute_price function and pass it as input\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# LSMC algorithm\n",
    "def compute_price(Θ, n, seed): # give size of simulation as input, because we're using batch_size 512 to compute loss function but we're using 100000 simulations to evaluate the quality of the output\n",
    "  S = Spot * jnp.ones(int(n))\n",
    "  rng_vector = jax.random.split(seed, int(m))\n",
    "  state = S\n",
    "  input = rng_vector #1st dimension of input needs to have same amount of steps to loop in the function\n",
    "  _, S = jax.lax.scan(step, state, input) #the S = 100 x 100000 matrix\n",
    "\n",
    "  discount = jnp.exp(-r * Δt)\n",
    "\n",
    "  # Very last date\n",
    "  value_if_exercise = payoff_put(S[-1])\n",
    "  discounted_future_cashflows = value_if_exercise * discount  \n",
    "\n",
    "\n",
    "  def core(state, input):\n",
    "    Si, Θi = input #we take in Θ as input because it keeps changing for every time step\n",
    "    discounted_future_cashflows = state\n",
    "\n",
    "    X = jnp.column_stack([Si**k for k in range(order)])\n",
    "    # X = chebyshev_basis(scale(Si), order)\n",
    "    Y = discounted_future_cashflows\n",
    "    # Θ = jnp.linalg.solve(X.T @ X, X.T @ Y) # not gonna compute regression\n",
    "    value_if_wait = X @ Θi # Theta becomes Theta i\n",
    "\n",
    "    mse_i = mse(value_if_wait, discounted_future_cashflows)\n",
    "\n",
    "    value_if_exercise = payoff_put(Si)\n",
    "    exercise = value_if_exercise >= value_if_wait\n",
    "    discounted_future_cashflows = discount * jnp.where(exercise, value_if_exercise, discounted_future_cashflows)\n",
    "\n",
    "    return discounted_future_cashflows, mse_i #we need to keep track of mse because we need minimize the sum and find mean of mse\n",
    "\n",
    "  input = jnp.flip(S[:-1], 0), Θ #pass theta as an input to core function because we need to pass diff Θ for every time step\n",
    "  state = discounted_future_cashflows\n",
    "  discounted_future_cashflows, mse_i = jax.lax.scan(core, state, input) #second output would be a vector of mse\n",
    "\n",
    "  # for i in range(m - 1):\n",
    "  #   discounted_future_cashflows, _ = core(discounted_future_cashflows, input[i])  \n",
    "  \n",
    "  return discounted_future_cashflows.mean(), mse_i\n",
    "\n",
    "# print(compute_price(Θ, n, seed)) #calculates the price using the guessed \n",
    "#each value in the 99 dimensional vector is the mse of the time stamp\n",
    "# test = compute_price(order, Spot, σ, K, r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IFFq6DYgKuQD"
   },
   "outputs": [],
   "source": [
    "import jax\n",
    "\n",
    "def L(Θ, seed):\n",
    "  price, mse_i = compute_price(Θ, batch_size, seed) #we're passing seed as input because each time we simulate the stock price we want a different random seed\n",
    "  return mse_i.sum()\n",
    "\n",
    "α=1e-5 # learning rate\n",
    "\n",
    "@jax.jit\n",
    "def update(Θ, opt_state, seed):\n",
    "  seed, _ = jax.random.split(seed) #split to make sure because we're using different seed when we're using it\n",
    "  grad = jax.grad(L)(Θ, seed)\n",
    "  updates, opt_state = optimizer(α).update(grad, opt_state, seed) # provide computed gradient and the previous optimizer state\n",
    "  Θ = optax.apply_updates(Θ, updates)\n",
    "  return Θ, opt_state, seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pnDD3fokKuQF"
   },
   "outputs": [],
   "source": [
    "for iteration in range(1000): #change range to 10000 and see what happens\n",
    "  Θ, opt_state, seed = update(Θ, opt_state, seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1669944206385,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "IR5ox-LDKuQF",
    "outputId": "24073cc0-c2b6-40f5-c158-7b3701006672"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray(3.9047234, dtype=float32),\n",
       " DeviceArray([25.9571    , 26.03179   , 28.30797   , 26.487484  ,\n",
       "              33.98196   , 23.261839  , 18.357145  , 20.911139  ,\n",
       "              18.861992  , 23.744764  , 37.110355  , 33.16365   ,\n",
       "              34.124043  , 32.42057   , 34.113705  , 32.352974  ,\n",
       "              23.001568  , 22.787683  , 18.880028  , 17.8579    ,\n",
       "              17.86499   , 23.699694  , 32.010426  , 26.327065  ,\n",
       "              28.527948  , 21.600445  , 23.341635  , 24.439177  ,\n",
       "              31.058313  , 31.720592  , 30.588894  , 27.225357  ,\n",
       "              25.545197  , 17.32303   , 15.062975  , 14.587344  ,\n",
       "              14.089913  , 14.731817  , 14.511595  , 14.353942  ,\n",
       "              13.672367  , 13.670407  , 14.524605  , 13.771536  ,\n",
       "              13.72872   , 14.093289  , 11.748952  , 13.548732  ,\n",
       "              11.669214  , 11.593536  , 13.253572  , 12.508336  ,\n",
       "              11.367655  , 11.479922  , 11.700112  , 11.503025  ,\n",
       "              10.911019  , 10.710645  , 11.72583   , 10.6823635 ,\n",
       "              13.342077  ,  9.583492  , 10.700317  ,  9.36899   ,\n",
       "               8.846708  ,  9.014905  ,  9.4946995 ,  8.297672  ,\n",
       "               7.7478647 ,  7.996611  ,  7.21308   ,  7.697351  ,\n",
       "               8.00344   ,  6.431494  ,  7.68608   ,  8.236123  ,\n",
       "               5.3244042 ,  5.8668804 ,  5.0341105 ,  5.051584  ,\n",
       "               4.8091598 ,  5.472526  ,  8.152554  ,  2.5555916 ,\n",
       "               3.3488522 ,  4.1158133 ,  2.5125844 ,  4.3458366 ,\n",
       "               3.3762481 ,  4.158312  ,  1.5937943 ,  1.890407  ,\n",
       "               0.89089185,  1.7410516 ,  3.9318027 ,  2.0437717 ,\n",
       "               0.5977631 ,  1.567389  ,  1.7507083 ], dtype=float32))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_price(Θ, n, seed) #we can observe the jump is option price because the Theta values are more optimized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CD1G_rNVb9Wo"
   },
   "source": [
    "5. Jax stochastic gradient descent neural network\n",
    "\n",
    "Why do we need neural networks? Because payoff can depend on a basket of stocks rather than only a single stock price\n",
    "\n",
    "we move from a 1D problem to a 3D problem\n",
    "\n",
    "First we make the NN model for 1 stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1670195117975,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "8_ChSyqYPXuZ"
   },
   "outputs": [],
   "source": [
    "# pip install optax dm-haiku\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import haiku as hk\n",
    "\n",
    "Spot = 36   # stock price\n",
    "σ = 0.2     # stock volatility\n",
    "K = 40      # strike price\n",
    "r = 0.06    # risk free rate\n",
    "n = 100000  # Number of simualted paths # we dont need so many paths if we are using stochastic gradient descent\n",
    "m = 100      # number of exercise dates\n",
    "batch_size = 512 #batch size \n",
    "#At each gd update we are sampling 512 data points to estimate the grade. In theory we need to use an infinite amount of data points\n",
    "\n",
    "T = 1       # maturity\n",
    "order = 5   # Polynmial order. Increase value and see what happens\n",
    "Δt = T / m  # interval between two exercise dates\n",
    "\n",
    "optimizer = optax.adam #we/re using optax to optimize gradient descent\n",
    "α=1e-3 # learning rate\n",
    "\n",
    "# We remove chebyshev because we arent using it\n",
    "\n",
    "# simulates one step of the stock price evolution\n",
    "def step(S, rng):\n",
    "  e = jax.random.normal(rng, shape=S.shape)\n",
    "  dB = jnp.sqrt(Δt) * e\n",
    "  S = S + r * S * Δt + σ * S * dB\n",
    "  return S, S\n",
    "\n",
    "def payoff_put(S): # S is a vector with different simulations for 1 time step\n",
    "                   # 100000 elements if gd or 512 if step\n",
    "  return jnp.maximum(K - S, 0.)\n",
    "\n",
    "\n",
    "def model(Si):\n",
    "  # out = jnp.column_stack([Si])\n",
    "  out = (jnp.column_stack([Si]) - 37)/5  # S is vector and haiku expects to be working on matrices. Sο S gets transformed to a matrix\n",
    "                                         # 37 is the mean and 5 is the std deviation\n",
    "                                         # we are normalizing the data\n",
    "                                         # see what happens to the predictions in the end if we dont normalize the data\n",
    "\n",
    "  out = hk.Linear(32)(out)\n",
    "  out = jax.nn.relu(out)\n",
    "  # out = jax.nn.silu(out) #this activation function doesnt have a problem when u dont normalize the data\n",
    "\n",
    "  out = hk.Linear(32)(out)\n",
    "  out = jax.nn.relu(out)\n",
    "  # out = jax.nn.silu(out)\n",
    "\n",
    "  out = hk.Linear(1)(out) # 512 x 1 matrix\n",
    "\n",
    "  return jnp.squeeze(out) # squeeze coverts to a vector\n",
    "\n",
    "init, model = hk.without_apply_rng(hk.transform(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "executionInfo": {
     "elapsed": 73,
     "status": "error",
     "timestamp": 1670195119054,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "xhGb1MtdRQMP",
    "outputId": "8c106f63-5981-4600-eb27-3ce85fe1adc4"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-28e21612279b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#stacking theta m - 1 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mΘ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-24-28e21612279b>\u001b[0m in \u001b[0;36mstack\u001b[0;34m(Θ)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#stacking theta m - 1 times\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mΘ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/lax_numpy.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(arrays, axis, out, dtype)\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1724\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1725\u001b[0;31m     \u001b[0m_stackable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_check_arraylike\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stack\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1726\u001b[0m     \u001b[0mshape0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1727\u001b[0m     \u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_canonicalize_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/jax/_src/numpy/util.py\u001b[0m in \u001b[0;36m_check_arraylike\u001b[0;34m(fun_name, *args)\u001b[0m\n\u001b[1;32m    343\u001b[0m                     if not _arraylike(arg))\n\u001b[1;32m    344\u001b[0m     \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"{} requires ndarray or scalar arguments, got {} at position {}.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfun_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: stack requires ndarray or scalar arguments, got <class 'dict'> at position 0."
     ]
    }
   ],
   "source": [
    "seed = jax.random.PRNGKey(0)\n",
    "Θ = init(seed, jnp.array(1.))\n",
    "opt_state = optimizer(α).init(Θ)\n",
    "\n",
    "\n",
    "def stack(Θ):\n",
    "  return jnp.stack([Θ] * (m-1))  #stacking theta m - 1 times\n",
    "\n",
    "Θ = stack(Θ)\n",
    "\n",
    "\n",
    "# Code wont work because stack only works on numpy arrays. Theta is not a Numpy Array. \n",
    "# What is Theta?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 141,
     "status": "ok",
     "timestamp": 1670190341020,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "TrPpeJjlbbss",
    "outputId": "4b813b17-cc18-4dbd-8d50-8f1153890f17"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670190341571,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "G4C3eOMhbbv5",
    "outputId": "15865e19-00be-4f11-f14d-195d0844a845"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['linear', 'linear_1', 'linear_2'])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ.keys()\n",
    "# Below is because we are doing 3 transformations in corresponding order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670190341715,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "4s10yYO8bbyu",
    "outputId": "f4f9a3ee-bdd5-498d-b040-bbf0d5ae9d8c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'w': DeviceArray([[ 0.3043298 , -0.20807579,  1.5253608 , -0.33471823,\n",
       "               -0.819684  ,  0.07262449,  0.75095516,  0.39394712,\n",
       "                1.1008915 , -0.08661517,  0.37275982,  0.62971044,\n",
       "               -1.2111528 ,  0.37929872,  0.16987085, -0.01042713,\n",
       "               -0.02237888, -0.25954646,  0.37912896,  0.35421863,\n",
       "                1.0106694 ,  0.49792686,  0.9558353 ,  1.2206774 ,\n",
       "               -1.1474493 ,  0.19136466, -0.3144515 ,  0.77004355,\n",
       "               -0.05240701,  0.6409038 ,  1.7368565 ,  0.2765501 ]],            dtype=float32),\n",
       " 'b': DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "              0., 0.], dtype=float32)}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ['linear']\n",
    "# 2 keys: w for weight; b for bias\n",
    "# w is a 1 x 32 matrix. (32 columns)\n",
    "# b is a vector with 32 elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIYN6aPKcF-M"
   },
   "source": [
    "Correct way to stack below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 296,
     "status": "ok",
     "timestamp": 1670195125928,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "t3H8ZjSqbb1w"
   },
   "outputs": [],
   "source": [
    "# Ideally we want to create a new theta that is a new dict of dict, \n",
    "# where every leaf will be a copy of the corresponding leaf of the original theta\n",
    "\n",
    "seed = jax.random.PRNGKey(0)\n",
    "Θ = init(seed, jnp.array(1.))\n",
    "opt_state = optimizer(α).init(Θ)\n",
    "\n",
    "\n",
    "def stack(Θ):\n",
    "  return jnp.stack([Θ] * (m-1))  #stacking theta m - 1 times\n",
    "\n",
    "# Θ = stack(Θ)\n",
    "Θ = jax.tree_map(stack, Θ)\n",
    "opt_state = optimizer(α).init(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1670190342106,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "z42q4Fs1nB_K",
    "outputId": "191d705f-7fda-4b8a-8515-312809b108ef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670190342267,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "BAD9ZHY8nGeR",
    "outputId": "02854327-c934-4fe0-f953-27c4afd99fa0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['linear', 'linear_1', 'linear_2'])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 293,
     "status": "ok",
     "timestamp": 1670190343120,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "wIP8NoxanQnG",
    "outputId": "77e03499-a17d-4fa3-b9ec-65f5ae0acf4c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['b', 'w'])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ['linear'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1670190343121,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "P2maZz3qZFco",
    "outputId": "b51d8bd1-5822-4192-f83e-9f799202baf7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 1, 32)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ['linear']['w'].shape\n",
    "# Now theta represents a collection(row stacked 99 times) of no_of_steps neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670190343594,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "GL7c3sCLnd-m",
    "outputId": "f9de1e2e-8ac4-4c39-d87e-101df4eaffc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(99, 32)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Θ['linear']['b'].shape\n",
    "# We can observe the first dimension to be 99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1670195128765,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "3xkzNX6jQJJj"
   },
   "outputs": [],
   "source": [
    "#Step 1 of grad descent\n",
    "# Θ = jnp.zeros((m-1, order)) \n",
    "# opt_state = optimizer(α).init(Θ)\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# LSMC algorithm\n",
    "def compute_price(Θ, n, seed): #pass seed because it is SGD\n",
    "  S = Spot * jnp.ones(n)\n",
    "  \n",
    "  rng_vector = jax.random.split(seed, int(m))\n",
    "  state = S\n",
    "  input = rng_vector \n",
    "  _, S = jax.lax.scan(step, state, input) \n",
    "\n",
    "  discount = jnp.exp(-r * Δt)\n",
    "\n",
    "  value_if_exercise = payoff_put(S[-1])\n",
    "  discounted_future_cashflows = value_if_exercise * discount\n",
    "\n",
    "  def core(state, input):\n",
    "    Si, Θi = input\n",
    "    discounted_future_cashflows = state\n",
    "    Y = discounted_future_cashflows\n",
    "\n",
    "    value_if_wait = model(Θi, Si) # using theta to calculate continouation value, \n",
    "                                  # ie how much dcf we expect to get given how much cashflow we'll get in the future\n",
    "                                  # if we find wrong value_if_wait, we will excercise when arent supposed to or vice versa\n",
    "                                  # actions are not optimal\n",
    "    \n",
    "    mse_i = jnp.mean((value_if_wait-discounted_future_cashflows)**2) \n",
    "    \n",
    "    value_if_exercise = payoff_put(Si)\n",
    "    exercise = value_if_exercise >= value_if_wait # decide if we want to excercise or not\n",
    "    \n",
    "    discounted_future_cashflows = discount * jnp.where(exercise, value_if_exercise, discounted_future_cashflows)\n",
    "\n",
    "    return discounted_future_cashflows, mse_i\n",
    "  \n",
    "  input = jnp.flip(S, 0)[1:], Θ\n",
    "  state = discounted_future_cashflows\n",
    "  discounted_future_cashflows, mse_i = jax.lax.scan(core, state, input)\n",
    "\n",
    "  return discounted_future_cashflows.mean(), mse_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "executionInfo": {
     "elapsed": 65,
     "status": "ok",
     "timestamp": 1670195129129,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "DqWZgxQBPXuc"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_gradient_descent(Θ, opt_state, seed): \n",
    "\n",
    "  seed, _ = jax.random.split(seed)\n",
    "\n",
    "  def L(Θ):\n",
    "    _, mse_i = compute_price(Θ, batch_size, seed)\n",
    "    return mse_i.sum()\n",
    "\n",
    "  grad = jax.grad(L)(Θ)\n",
    "  updates, opt_state = optimizer(α).update(grad, opt_state) \n",
    "                                                            \n",
    "  \n",
    "  Θ = optax.apply_updates(Θ, updates)\n",
    "  return Θ, opt_state, seed \n",
    "\n",
    "@jax.jit\n",
    "def evaluate(Θ):\n",
    "  seed = jax.random.PRNGKey(0) #doesnt matter if we use the same seed for evaluation\n",
    "  p, _ = compute_price(Θ, 200000, seed)\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 86872,
     "status": "error",
     "timestamp": 1670195216672,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "dySlRcwHBQ2f",
    "outputId": "7fcf1037-3a98-412a-adf3-1e62b1ccdc09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.9746199\n",
      "4.220788\n",
      "4.2116103\n",
      "4.236565\n",
      "4.2453923\n",
      "4.227541\n",
      "4.262583\n",
      "4.2285404\n",
      "4.2582145\n",
      "4.2047067\n",
      "4.219553\n",
      "4.2235665\n",
      "4.215366\n",
      "4.1404123\n",
      "4.159133\n",
      "4.204374\n",
      "4.1307473\n",
      "4.1960907\n",
      "4.145729\n",
      "4.2015853\n",
      "4.1845417\n",
      "4.1999936\n",
      "4.1738873\n",
      "4.127784\n",
      "4.055139\n",
      "4.16718\n",
      "4.1879697\n",
      "4.1982393\n",
      "4.2164335\n",
      "4.1763687\n",
      "4.1564307\n",
      "4.134822\n",
      "4.1399903\n",
      "4.1595397\n",
      "4.2170157\n",
      "4.2179804\n",
      "4.1733956\n",
      "4.188957\n",
      "4.0413756\n",
      "4.1784067\n",
      "4.0953817\n",
      "4.192585\n",
      "4.1882777\n",
      "4.232761\n",
      "4.2434406\n",
      "4.1993184\n",
      "4.2331095\n",
      "4.165975\n",
      "4.164168\n",
      "4.1043935\n",
      "4.1734076\n",
      "4.24499\n",
      "4.1056356\n",
      "4.1809583\n",
      "4.1289825\n",
      "4.254113\n",
      "4.1966076\n",
      "4.1711965\n",
      "4.204183\n",
      "4.077337\n",
      "4.112798\n",
      "4.164596\n",
      "4.0749545\n",
      "4.1415453\n",
      "4.191174\n",
      "4.236075\n",
      "4.168531\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-00cebd1d7076>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = jax.random.PRNGKey(0)\n",
    "\n",
    "for iteration in range(1000000):\n",
    "  Θ, opt_state, seed = update_gradient_descent(Θ, opt_state, seed)\n",
    "\n",
    "  if iteration % 100 == 0:\n",
    "    print(evaluate(Θ))\n",
    "\n",
    "# We improved found the model by normalizing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qV06QimCIviT"
   },
   "source": [
    "Now we need to modify our step function and payoff function to handle more than 1 stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 66,
     "status": "ok",
     "timestamp": 1670192793486,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "fCXURlHbHuWy"
   },
   "outputs": [],
   "source": [
    "# pip install optax dm-haiku\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import haiku as hk\n",
    "\n",
    "Spot = jnp.array([38, 36, 35])   # stock , no longer scalar, it is a vector\n",
    "σ = jnp.array([.2, .25, 0.3])    # we can give volatility for each stock     \n",
    "K = 40      \n",
    "r = 0.06    \n",
    "n = 512  \n",
    "m = 100     \n",
    "\n",
    "T = 1       \n",
    "order = 5   \n",
    "Δt = T / m  \n",
    "\n",
    "optimizer = optax.adam\n",
    "α=1e-3\n",
    "\n",
    "# no we need to change this function to take in multiple stock values\n",
    "def step(S, rng):\n",
    "  e = jax.random.normal(rng, shape=S.shape) # we dont need to change this line because we wrote it in an agnostic way.\n",
    "                                            # If S is a matrix, e will be a matrix\n",
    "  dB = jnp.sqrt(Δt) * e\n",
    "  S = S + r * S * Δt + σ * S * dB\n",
    "  return S, S\n",
    "\n",
    "# we need to change the payoff\n",
    "def payoff_put(S):\n",
    "  return jnp.maximum(K - jnp.max(S, axis=1), 0.) #axis is 1 because we ned the sum of column. reason is in lecture notes written in the end\n",
    "\n",
    "def model(Si):\n",
    "  out = (Si - 37)/5.  # we remove column stack because S is a matrix \n",
    "                      # we can scale each row with a respective mean and td deviation?\n",
    "  out = hk.Linear(32)(out)\n",
    "  out = jax.nn.relu(out)\n",
    "\n",
    "  out = hk.Linear(32)(out)\n",
    "  out = jax.nn.relu(out)\n",
    "\n",
    "  out = hk.Linear(1)(out) #512 x 1 matrix\n",
    "\n",
    "  return jnp.squeeze(out)\n",
    "\n",
    "init, model = hk.without_apply_rng(hk.transform(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 2506,
     "status": "ok",
     "timestamp": 1670192969512,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "hY7B4HOTHuW3"
   },
   "outputs": [],
   "source": [
    "seed = jax.random.PRNGKey(0)\n",
    "Θ = init(seed, jnp.zeros((512, 3))) #we need to give a sample of your inputs \n",
    "#why do we change it like this?\n",
    "\n",
    "opt_state = optimizer(α).init(Θ)\n",
    "\n",
    "def stack(Θ):\n",
    "  return jnp.stack([Θ] * (m-1))  #stacking theta m - 1 times\n",
    "\n",
    "Θ = jax.tree_map(stack, Θ)\n",
    "opt_state = optimizer(α).init(Θ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1670192969513,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "1VhzSC2SK2Nc",
    "outputId": "d425c8db-1974-4bb3-b795-6ea9adedc357"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(512, 3)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# n = 512\n",
    "S = jnp.column_stack([Spot[i] * jnp.ones(n) for i in range(3)])\n",
    "S.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 86,
     "status": "ok",
     "timestamp": 1670192972369,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "VOk63KNyHuW6"
   },
   "outputs": [],
   "source": [
    "# @jax.jit\n",
    "# LSMC algorithm\n",
    "def compute_price(Θ, n, seed): #pass seed because it is SGD\n",
    "  # S = Spot * jnp.ones(n, 3) # S is a matrix with different simulations in 1 time step for 3 stocks \n",
    "  S = jnp.column_stack([Spot[i] * jnp.ones(n) for i in range(3)])\n",
    "  \n",
    "  rng_vector = jax.random.split(seed, int(m))\n",
    "  state = S\n",
    "  input = rng_vector \n",
    "  _, S = jax.lax.scan(step, state, input) \n",
    "\n",
    "  discount = jnp.exp(-r * Δt)\n",
    "\n",
    "  value_if_exercise = payoff_put(S[-1])\n",
    "  discounted_future_cashflows = value_if_exercise * discount\n",
    "\n",
    "  def core(state, input):\n",
    "    Si, Θi = input\n",
    "    discounted_future_cashflows = state\n",
    "    Y = discounted_future_cashflows\n",
    "\n",
    "    value_if_wait = model(Θi, Si)\n",
    "    \n",
    "    mse_i = jnp.mean((value_if_wait-discounted_future_cashflows)**2) \n",
    "    \n",
    "    value_if_exercise = payoff_put(Si)\n",
    "    exercise = value_if_exercise >= value_if_wait\n",
    "    \n",
    "    discounted_future_cashflows = discount * jnp.where(exercise, value_if_exercise, discounted_future_cashflows)\n",
    "\n",
    "    return discounted_future_cashflows, mse_i\n",
    "  \n",
    "  input = jnp.flip(S, 0)[1:], Θ\n",
    "  state = discounted_future_cashflows\n",
    "  discounted_future_cashflows, mse_i = jax.lax.scan(core, state, input)\n",
    "\n",
    "  return discounted_future_cashflows.mean(), mse_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1670192972958,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "bsc5iD4nHuW6"
   },
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_gradient_descent(Θ, opt_state, seed): \n",
    "\n",
    "  seed, _ = jax.random.split(seed)\n",
    "\n",
    "  def L(Θ):\n",
    "    _, mse_i = compute_price(Θ, batch_size, seed)\n",
    "    return mse_i.sum()\n",
    "\n",
    "  grad = jax.grad(L)(Θ)\n",
    "  updates, opt_state = optimizer(α).update(grad, opt_state) \n",
    "                                                            \n",
    "  \n",
    "  Θ = optax.apply_updates(Θ, updates)\n",
    "  return Θ, opt_state, seed \n",
    "\n",
    "@jax.jit\n",
    "def evaluate(Θ):\n",
    "  seed = jax.random.PRNGKey(0) #doesnt matter if we use the same seed for evaluation\n",
    "  p, _ = compute_price(Θ, 200000, seed)\n",
    "  return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 128962,
     "status": "error",
     "timestamp": 1670193102376,
     "user": {
      "displayName": "Hariharan Manickam",
      "userId": "04777628255365769847"
     },
     "user_tz": 360
    },
    "id": "B-v1ADPjHuW6",
    "outputId": "15853956-abce-463c-d340-8484b060a5db"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.9517\n",
      "2.1202378\n",
      "2.1510992\n",
      "2.206948\n",
      "2.2358725\n",
      "2.2463207\n",
      "2.2508252\n",
      "2.2458668\n",
      "2.2445405\n",
      "2.2517066\n",
      "2.2541823\n",
      "2.2480638\n",
      "2.2572012\n",
      "2.2468977\n",
      "2.2545292\n",
      "2.2505789\n",
      "2.2537115\n",
      "2.2479827\n",
      "2.2452836\n",
      "2.2520442\n",
      "2.2537427\n",
      "2.2495224\n",
      "2.258139\n",
      "2.2514815\n",
      "2.2507145\n",
      "2.2499056\n",
      "2.2546976\n",
      "2.2589822\n",
      "2.2518132\n",
      "2.2558422\n",
      "2.256078\n",
      "2.2534692\n",
      "2.2586975\n",
      "2.2520983\n",
      "2.2599974\n",
      "2.2616544\n",
      "2.2565858\n",
      "2.2609622\n",
      "2.2493684\n",
      "2.25171\n",
      "2.257472\n",
      "2.2449715\n",
      "2.2563703\n",
      "2.2483518\n",
      "2.2533047\n",
      "2.2505329\n",
      "2.2511134\n",
      "2.257806\n",
      "2.258041\n",
      "2.2498991\n",
      "2.2564812\n",
      "2.256799\n",
      "2.2604113\n",
      "2.2494233\n",
      "2.2538314\n",
      "2.2359686\n",
      "2.2526343\n",
      "2.2567616\n",
      "2.2539997\n",
      "2.2525725\n",
      "2.2586255\n",
      "2.257685\n",
      "2.251334\n",
      "2.2598798\n",
      "2.2527988\n",
      "2.2535276\n",
      "2.2615912\n",
      "2.261707\n",
      "2.2576766\n",
      "2.2545316\n",
      "2.2591352\n",
      "2.2526622\n",
      "2.2577872\n",
      "2.2570925\n",
      "2.253807\n",
      "2.250126\n",
      "2.2581263\n",
      "2.2602859\n",
      "2.2625966\n",
      "2.2501795\n",
      "2.2544734\n",
      "2.2573736\n",
      "2.2579818\n",
      "2.253785\n",
      "2.2643669\n",
      "2.2611625\n",
      "2.2545593\n",
      "2.2496173\n",
      "2.2473187\n",
      "2.2593272\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-4b3ed958136b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mupdate_gradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mΘ\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0miteration\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<string>\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "seed = jax.random.PRNGKey(0)\n",
    "\n",
    "for iteration in range(1000000):\n",
    "  Θ, opt_state, seed = update_gradient_descent(Θ, opt_state, seed)\n",
    "\n",
    "  if iteration % 100 == 0:\n",
    "    print(evaluate(Θ))\n",
    "\n",
    "# Correct answer is 2.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FpaImPLHuW7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9qoj-2RhTG0S"
   },
   "source": [
    "In normal equations, we scale because each varaible will have its own scale.\n",
    "\n",
    "In NN the "
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [
    {
     "file_id": "1eSNNm__6qp29c0nd7Y5O5-P80-8VmyFj",
     "timestamp": 1669940845041
    },
    {
     "file_id": "1gscjwwbumqytfWrCQIswei0FN0vUYmVl",
     "timestamp": 1669884705142
    },
    {
     "file_id": "1SCLPuKK0vqkiBwe763RWRUolQkZ89PZP",
     "timestamp": 1665598212845
    }
   ]
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
