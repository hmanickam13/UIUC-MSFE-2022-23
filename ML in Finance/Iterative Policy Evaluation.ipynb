{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1KYvD5dGmnWbSwJldHl_dK86O-WW9nz4m","timestamp":1671088372105},{"file_id":"1ioyp9OX4H9wa1zO9EvdAr4N7CEMyeMP6","timestamp":1669889176516}]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","source":["Due Date: December 7th"],"metadata":{"id":"F3ZF3oYIJ3KW"}},{"cell_type":"markdown","metadata":{"id":"YzptQhdh0pUr"},"source":["# Vaccine Development with Dynamic Programming\n","\n","You are the CEO of a biotech company which is considering the development of a new vaccine. Starting at phase 0 (state 0), the drug develpment can stay in the same state or advance to \"phase 1  with promising results\" (state 1) or advance to \"phase 1 with disappointing results\" (state 2), or fail completely (state 4). At phase 1, the drug can stay in the same state, fail or become a success (state 3), in which case you will sell its patent to a big pharma company for \\$10 million.\n","These state transitions happen from month to month, and at each state, you have the option to make an additional investment of \\$100,000, which increases the chances of success.\n","\n","After careful study, your analysts develop the program below to simulate different scenarios using statistical data from similar projects. \n","\n","Use a discount factor of 0.996.\n","\n","- 1) Write a policy iteration algorithm to compute the value of this project. Please print the full V vector.\n","\n","- 2 )Write a value iteration algorithm to compute the value of this project. Please print the full V vector."]},{"cell_type":"code","metadata":{"id":"dnAvrShs6ecs","executionInfo":{"status":"ok","timestamp":1671141345117,"user_tz":360,"elapsed":12,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}}},"source":["import numpy as np\n","\n","class MDP():\n","  def __init__(self):\n","    self.A = [0, 1] # action matrix (list)\n","    self.S = [0, 1, 2, 3, 4] # state matrix (list)\n","\n","    # Transition matrix when we dont invest\n","    P0 = np.array([[0.5, .15, .15, 0, .20], # staying the same\n","                   [0, .5, .0, .25, .25],   # promising results\n","                   [0, 0, .15, .05, .8],    # dissapointing results\n","                   [0, 0, 0, 0, 1],         # success\n","                   [0, 0, 0, 0, 1]])        # fail completely\n","    # Reward when we dont invest\n","    R0 = np.array([0, 0, 0, 10, 0])\n","\n","    # Transition matrix when we invest\n","    P1 = np.array([[0.5, .25, .15, 0, .10], # staying the same\n","                   [0, .5, .0, .35, .15],   # promising results\n","                   [0, 0, .20, .05, .75],   # dissapointing results\n","                   [0, 0, 0, 0, 1],         # success\n","                   [0, 0, 0, 0, 1]])        # fail completely\n","\n","    # Reward when we invest\n","    R1 = np.array([-0.1, -0.1, -0.1, 10, 0]) \n","\n","    self.P = [P0, P1] # stacking as lists\n","    self.R = [R0, R1] # stacking as lists\n","\n","  # given current state and transition probability, this function \n","  # predcits the random choice we make to transition from one state to another.\n","  def step(self, s, a): # s is current state, a is chosen action\n","    s_prime = np.random.choice(len(self.S), p=self.P[a][s]) # next random state predicted using the given probability distribution\n","    R = self.R[a][s] # finds reward function\n","    if s_prime == 4: # checks if the next choice was failure\n","      done = True \n","    else:\n","      done = False\n","    return s_prime, R, done # Returns s_prime(scalar): the next choice we make\n","                            # Returns R(scalar): rewardwe get we take the next step\n","                            # returns done(bool): true if the next step was failure \n","\n","# function simulating next steps\n","# takes as input self, s: current state, a: chosen action, π : probabiity matrix which depends on s\n","  def simulate(self, s, a, π): \n","    done = False # Boolean set to false implying not failed\n","    t = 0 # starts at time 0\n","    history = [] # list\n","    while not done: # as long as company didnt fail\n","      if t > 0: # when time is greater than 0\n","        a = π[s] # action = transition row(probability) for current state\n","      s_prime, R, done = self.step(s, a) # takes in inputs\n","                                         # returns next state(scalar), reward(scalar), failed(bool)\n","      history.append((s, a, R)) # adding to list current state, chosen action, reward\n","      s = s_prime # next state becomes current state\n","      t += 1 # time increments by 1\n","      print(t)\n","    \n","    return history # returning the list containing all choices"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":["You can access the transition probability matrices and the reward vector as follows:"],"metadata":{"id":"xgAiJxSnZtkH"}},{"cell_type":"code","metadata":{"id":"5-rfjh_37kmX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671141345117,"user_tz":360,"elapsed":10,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}},"outputId":"7549f3e1-c041-4a69-8d81-f57130edb928"},"source":["mdp = MDP()\n","P = mdp.P\n","R = mdp.R\n","\n","\n","s = 2 # current state\n","s_prime = 4  # next state\n","a = 1  # chosen action\n","\n","# Probability of transition from state s (2) to s_prime (4) if action == a (1):\n","print(P[a][s, s_prime])\n","\n","# Reward at state s if action = a\n","print(R[a][s])"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["0.75\n","-0.1\n"]}]},{"cell_type":"code","source":["# rough work\n","π = [0, 1, 0, 1, 0] # initial policy\n","\n","res = mdp.simulate(s, a, π)\n","res"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FMWf5-x0pBvM","executionInfo":{"status":"ok","timestamp":1671141345117,"user_tz":360,"elapsed":8,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}},"outputId":"7e2a753b-5f67-4413-a12c-d83b53e5a393"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n"]},{"output_type":"execute_result","data":{"text/plain":["[(2, 1, -0.1)]"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["## 1. Solution for Policy Iteration"],"metadata":{"id":"N8x0fwbEwC05"}},{"cell_type":"code","source":["import numpy as np\n","\n","class MDP():\n","  def __init__(self, γ, π, Vπ, π_vec):\n","    self.A = [0, 1]          # action matrix (list)\n","    self.S = [0, 1, 2, 3, 4] # state matrix (list)\n","    self.γ = γ               # discount function\n","    self.π = π               # [0, 0, 0, 0, 0] # initial policy\n","    self.Vπ = Vπ             # initial guess for Vπ\n","    self.π_vec = π_vec\n","\n","    π = [0, 0, 0, 0, 0]\n","\n","    # Transition matrix when we dont invest\n","    P0 = np.array([[0.5, .15, .15, 0, .20], # staying the same\n","                   [0, .5, .0, .25, .25],   # promising results\n","                   [0, 0, .15, .05, .8],    # dissapointing results\n","                   [0, 0, 0, 0, 1],         # success\n","                   [0, 0, 0, 0, 1]])        # fail completely\n","    # Reward when we dont invest\n","    R0 = np.array([0, 0, 0, 10, 0])\n","\n","    # Transition matrix when we invest\n","    P1 = np.array([[0.5, .25, .15, 0, .10], # staying the same\n","                   [0, .5, .0, .35, .15],   # promising results\n","                   [0, 0, .20, .05, .75],   # dissapointing results\n","                   [0, 0, 0, 0, 1],         # success\n","                   [0, 0, 0, 0, 1]])        # fail completely\n","\n","    # Reward when we invest\n","    R1 = np.array([-0.1, -0.1, -0.1, 10, 0]) \n","\n","    self.P = [P0, P1] # stacking as lists\n","    self.R = [R0, R1] # stacking as lists\n","\n","  # given current state and transition probability, this function \n","  # predcits the random choice we make to transition from one state to another.\n","  def step(self, s, a):                                     # s is current state, a is chosen action\n","    s_prime = np.random.choice(len(self.S), p=self.P[a][s]) # next random state predicted using the given probability distribution\n","    R = self.R[a][s]                                        # finds reward function\n","    if s_prime == 4:                                        # checks if the next choice was failure\n","      done = True \n","    else:\n","      done = False\n","    return s_prime, R, done # Returns s_prime(scalar): the next choice we make\n","                            # Returns R(scalar): rewardwe get we take the next step\n","                            # returns done(bool): true if the next step was failure \n","\n","# function simulating next steps\n","# takes as input self, s: current state, a: chosen action, π : probabiity matrix which depends on s\n","  def simulate(self, s, a, π): \n","    done = False                         # Boolean set to false implying not failed\n","    t = 0                                # starts at time 0\n","    history = []                         # list\n","    while not done:                      # as long as company didnt fail\n","      if t > 0:                          # when time is greater than 0\n","        a = π[s]                         # action = transition row(probability) for current state\n","      s_prime, R, done = self.step(s, a) # takes in inputs\n","                                         # returns next state(scalar), reward(scalar), failed(bool)\n","      history.append((s, a, R))          # adding to list current state, chosen action, reward\n","      s = s_prime                        # next state becomes current state\n","      t += 1                             # time increments by 1\n","      print(t)\n","    \n","    return history                       # returning the list containing all choices\n","\n","  # constructing the implied reward function\n","  # referred to sample codes\n","  def construct_Rπ(self, R, π, S):\n","    Rπ = np.zeros(len(S))\n","    for s in S:\n","      Rπ[s] = R[π[s]][s]\n","    return Rπ\n","\n","  # constructing the implied transition matrix for all states and actions\n","  # referred to sample codes\n","  def construct_Pπ(self, P, π, S):\n","    Pπ = np.zeros((len(S), len(S)))\n","    for s in S:\n","      for s_prime in S:\n","        Pπ[s, s_prime] = P[π[s]][s, s_prime]\n","    return Pπ\n","\n","  # Iterative policy evaluation\n","  # referred to sample codes\n","  def policy_evaluation(self, π, Vπ, S, γ):\n","    Rπ = self.construct_Rπ(R, π, S)\n","    Pπ = self.construct_Pπ(P, π, S)\n","    for iteration in range(10000): # 1 step policy iteration is called value iteration. 10000 becomes 10 times or once\n","      Vπ = Rπ + γ * Pπ @ Vπ\n","    # print(Vπ)\n","    return Vπ\n","\n","  # Policy improvement\n","  # referred to sample codes\n","  def policy_improvement(self, Vπ, S, A, γ):\n","    # Compute Qπ using Vπ\n","    Qπ = np.zeros((5, 2))\n","    π_prime = np.zeros(5, dtype=np.int32)\n","    for s in S:\n","      for a in A:\n","        # Qπ is what i expect to get when i am in state s, and i do the action a, and from tmr i'm gonna follow the policy π\n","        Qπ[s, a] = R[a][s] + γ * P[a][s] @ Vπ # page 15 lec 14 highlighted diff Bellmann eqn\n","                                              # we know Vπ after policy evaluation\n","\n","    # Greedy updates\n","    # referred to sample codes\n","    for s in S:\n","      π_prime[s] = np.argmax(Qπ[s, :]) # check diff belman eqn derivation in notes to understand\n","    return π_prime\n","\n","  # policy iteration\n","  def loop(self, π, Vπ, S, A, γ, π_vec, R, P):\n","    count = 0\n","    for i in range(10000):\n","      Vπ = self.policy_evaluation(π, Vπ, S, γ)\n","      π = self.policy_improvement(Vπ, S, A, γ)\n","      π_vec.append(π)\n","      \n","      if i>2:\n","        if np.array_equal(π_vec[i],π_vec[i-1]):\n","          count=count+1\n","        else:\n","          count = 0  \n","      \n","      if count == 30:\n","\n","        return Vπ, π_vec"],"metadata":{"id":"aJ7-UacepBs1","executionInfo":{"status":"ok","timestamp":1671141345118,"user_tz":360,"elapsed":4,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["π = [0, 0, 0, 0, 0] # initial policy guess for π\n","Vπ = np.zeros(5)    # initial Vπ\n","π_vec = []          # vector that\n","γ = .996            # discount function\n","\n","mdp = MDP(γ, π, Vπ, π_vec) # iniatilising class MDP\n","\n","A = mdp.A   # calling the variables outside th class to call it in the loop function\n","S = mdp.S\n","R = mdp.R\n","P = mdp.P\n","\n","x, y = mdp.loop(π, Vπ, S, A, γ, π_vec, R, P) # calling loop to perform policy iteration"],"metadata":{"id":"Ce9LcG0iiI5S","executionInfo":{"status":"ok","timestamp":1671141346359,"user_tz":360,"elapsed":1245,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"pWfK-47V8I08","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671141346359,"user_tz":360,"elapsed":12,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}},"outputId":"1d6a51cb-6ebc-4c9a-8504-c25cea275260"},"source":["x # Vπ"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3.32067538,  6.74501992,  0.58546908, 10.        ,  0.        ])"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["\n","### Above is the V vector for policy iteration"],"metadata":{"id":"xamosCQDwJWm"}},{"cell_type":"code","source":["# y # π_vec"],"metadata":{"id":"HO7QOSoC2eTV","executionInfo":{"status":"ok","timestamp":1671141346359,"user_tz":360,"elapsed":8,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## 2. Solution for Value Iteration"],"metadata":{"id":"1qKatp4VwphC"}},{"cell_type":"code","source":["import numpy as np\n","\n","class MDP():\n","  def __init__(self, γ, π, Vπ, π_vec):\n","    self.A = [0, 1]          # action matrix (list)\n","    self.S = [0, 1, 2, 3, 4] # state matrix (list)\n","    self.γ = γ               # discount function\n","    self.π = π               # [0, 0, 0, 0, 0] # initial policy\n","    self.Vπ = Vπ             # initial guess for Vπ\n","    self.π_vec = π_vec\n","\n","    π = [0, 0, 0, 0, 0]\n","\n","    # Transition matrix when we dont invest\n","    P0 = np.array([[0.5, .15, .15, 0, .20], # staying the same\n","                   [0, .5, .0, .25, .25],   # promising results\n","                   [0, 0, .15, .05, .8],    # dissapointing results\n","                   [0, 0, 0, 0, 1],         # success\n","                   [0, 0, 0, 0, 1]])        # fail completely\n","    # Reward when we dont invest\n","    R0 = np.array([0, 0, 0, 10, 0])\n","\n","    # Transition matrix when we invest\n","    P1 = np.array([[0.5, .25, .15, 0, .10], # staying the same\n","                   [0, .5, .0, .35, .15],   # promising results\n","                   [0, 0, .20, .05, .75],   # dissapointing results\n","                   [0, 0, 0, 0, 1],         # success\n","                   [0, 0, 0, 0, 1]])        # fail completely\n","\n","    # Reward when we invest\n","    R1 = np.array([-0.1, -0.1, -0.1, 10, 0]) \n","\n","    self.P = [P0, P1] # stacking as lists\n","    self.R = [R0, R1] # stacking as lists\n","\n","  # given current state and transition probability, this function \n","  # predcits the random choice we make to transition from one state to another.\n","  def step(self, s, a):                                     # s is current state, a is chosen action\n","    s_prime = np.random.choice(len(self.S), p=self.P[a][s]) # next random state predicted using the given probability distribution\n","    R = self.R[a][s]                                        # finds reward function\n","    if s_prime == 4:                                        # checks if the next choice was failure\n","      done = True \n","    else:\n","      done = False\n","    return s_prime, R, done # Returns s_prime(scalar): the next choice we make\n","                            # Returns R(scalar): rewardwe get we take the next step\n","                            # returns done(bool): true if the next step was failure \n","\n","# function simulating next steps\n","# takes as input self, s: current state, a: chosen action, π : probabiity matrix which depends on s\n","  def simulate(self, s, a, π): \n","    done = False                         # Boolean set to false implying not failed\n","    t = 0                                # starts at time 0\n","    history = []                         # list\n","    while not done:                      # as long as company didnt fail\n","      if t > 0:                          # when time is greater than 0\n","        a = π[s]                         # action = transition row(probability) for current state\n","      s_prime, R, done = self.step(s, a) # takes in inputs\n","                                         # returns next state(scalar), reward(scalar), failed(bool)\n","      history.append((s, a, R))          # adding to list current state, chosen action, reward\n","      s = s_prime                        # next state becomes current state\n","      t += 1                             # time increments by 1\n","      print(t)\n","    \n","    return history                       # returning the list containing all choices\n","\n","  # constructing the implied reward function\n","  # referred to sample codes\n","  def construct_Rπ(self, R, π, S):\n","    Rπ = np.zeros(len(S))\n","    for s in S:\n","      Rπ[s] = R[π[s]][s]\n","    return Rπ\n","\n","  # constructing the implied transition matrix for all states and actions\n","  # referred to sample codes\n","  def construct_Pπ(self, P, π, S):\n","    Pπ = np.zeros((len(S), len(S)))\n","    for s in S:\n","      for s_prime in S:\n","        Pπ[s, s_prime] = P[π[s]][s, s_prime]\n","    return Pπ\n","\n","  # Iterative policy evaluation\n","  # referred to sample codes\n","  def policy_evaluation(self, π, Vπ, S, γ):\n","    Rπ = self.construct_Rπ(R, π, S)\n","    Pπ = self.construct_Pπ(P, π, S)\n","    for iteration in range(1): # 1 step policy iteration is called value iteration. 10000 becomes 10 times or once\n","      Vπ = Rπ + γ * Pπ @ Vπ\n","    # print(Vπ)\n","    return Vπ\n","\n","  # Policy improvement\n","  # referred to sample codes\n","  def policy_improvement(self, Vπ, S, A, γ):\n","    # Compute Qπ using Vπ\n","    Qπ = np.zeros((5, 2))\n","    π_prime = np.zeros(5, dtype=np.int32)\n","    for s in S:\n","      for a in A:\n","        # Qπ is what i expect to get when i am in state s, and i do the action a, and from tmr i'm gonna follow the policy π\n","        Qπ[s, a] = R[a][s] + γ * P[a][s] @ Vπ # page 15 lec 14 highlighted diff Bellmann eqn\n","                                              # we know Vπ after policy evaluation\n","\n","    # Greedy updates\n","    # referred to sample codes\n","    for s in S:\n","      π_prime[s] = np.argmax(Qπ[s, :]) # check diff belman eqn derivation in notes to understand\n","    return π_prime\n","\n","  # value iteration\n","  def loop(self, π, Vπ, S, A, γ, π_vec, R, P):\n","    count = 0\n","    for i in range(10000):\n","      Vπ = self.policy_evaluation(π, Vπ, S, γ)\n","      π = self.policy_improvement(Vπ, S, A, γ)\n","      π_vec.append(π)\n","      \n","      if i>2:\n","        if np.array_equal(π_vec[i],π_vec[i-1]):\n","          count=count+1\n","        else:\n","          count = 0  \n","      \n","      if count == 30:\n","\n","        return Vπ, π_vec"],"metadata":{"id":"D2cfvm5udE8-","executionInfo":{"status":"ok","timestamp":1671141346360,"user_tz":360,"elapsed":9,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["π = [0, 0, 0, 0, 0] # initial policy guess for π\n","Vπ = np.zeros(5)    # initial Vπ\n","π_vec = []          # vector that\n","γ = .996            # discount function\n","\n","mdp = MDP(γ, π, Vπ, π_vec) # iniatilising class MDP\n","\n","A = mdp.A   # calling the variables outside th class to call it in the loop function\n","S = mdp.S\n","R = mdp.R\n","P = mdp.P\n","\n","x, y = mdp.loop(π, Vπ, S, A, γ, π_vec, R, P) # calling loop to perform policy iteration"],"metadata":{"executionInfo":{"status":"ok","timestamp":1671141346360,"user_tz":360,"elapsed":8,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}},"id":"ud6iqqJawaDR"},"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1671141346710,"user_tz":360,"elapsed":358,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}},"outputId":"36144e83-6ac1-44e3-c992-26e090ddd70c","id":"Fo2snKFWwaDS"},"source":["x # Vπ"],"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 3.32067536,  6.74501992,  0.58546908, 10.        ,  0.        ])"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["\n","### Above is the V vector for value iteration"],"metadata":{"id":"itFN2OvuwaDS"}},{"cell_type":"code","source":["# y # π_vec"],"metadata":{"executionInfo":{"status":"ok","timestamp":1671141346711,"user_tz":360,"elapsed":354,"user":{"displayName":"Hariharan Manickam","userId":"04777628255365769847"}},"id":"v4J_JvH2waDS"},"execution_count":11,"outputs":[]}]}